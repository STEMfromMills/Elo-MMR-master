
\section{Conclusions}
This paper introduces the Elo-MMR rating system, which is in part a generalization of the two-player Glicko system, allowing any number of players. By developing a Bayesian model and taking the limit as the number of participants goes to infinity, we obtained simple, human-interpretable rating update formulas. Furthermore, we saw that the algorithm is incentive-compatible, robust to extreme performances, asymptotically fast, and embarrassingly parallel. To our knowledge, our system is the first to rigorously prove all these properties in a setting with more than two individually ranked players. In terms of practical performance, we saw that it outperforms existing industry systems in both prediction accuracy and computation speed.
%In particular, we compare against the popular CodeForces, Topcoder, and TrueSkill rating systems, which are deployed on platforms with hundreds of thousands to millions of users.

This work can be extended in several directions. First, the choices we made in modeling ties, pseudodiffusions, and opponent subsampling are by no means the only possibilities consistent with our Bayesian model of skills and performances. Second, it may be possible to further improve accuracy by fitting more flexible performance and skill evolution models to application-specific data.

Another useful extension would be to team competitions. Given a performance model for teams, Elo-MMR infers each team's performance. To make this useful in settings where teams are frequently reassigned, we must model teams in terms of their individual members; unfortunately, it's not possible to precisely infer an individual's performance from team rankings alone. Therefore, it becomes necessary to condition an individual's skill on their team's performance. In the case where a team's performance is modeled as the sum of its members' independent Gaussian contributions, elementary facts about multivariate Gaussian distributions enable posterior skill inferences at the individual level. Generalizing this approach to other models remains an open challenge.

% Probably redundant: The algorithm itself is trivially parallelizable, and further speedup can be attained through a simple sub-sampling strategy. We believe there is potential to improve the performance even more, either through a more sophisticated sub-sampling strategy, interpolation, or by combining our two-phase approach with a factor graph framework similar to that of TrueSkill~\cite{HMG06, KFL01}. 

Over the past decade, online competition communities such as Codeforces have grown exponentially. As such, considerable work has gone into engineering scalable and reliable rating systems. Unfortunately, many of these systems have not been rigorously analyzed in the academic community. We hope that our paper and open-source release will open new explorations in this area.

%In addition, we invite non-technical sporting communities, such as the Spartan Race and DanceSport, to find uses of our skill estimation package.