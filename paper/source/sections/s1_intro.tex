
\section{Introduction}

% \begin{comment}
% Notes to self:
% - Should we do all-pairs ELO/Glicko per contest as a comparison?
%     - Performances where you do really well is amplified (\#1 is treated as O(n) wins where n is the number of competitors)
% - Can we prove the "aligned incentives" property? I.e. is a participant ever incentivized to lose?
%     - Can show that edits to past performance is monotonic w.r.t. to the edits

% Important points in introduction:
% - Rating systems are ad-hoc, preferences for algorithms based mostly on word-of-mouth and first movers.
% - Different domains use different systems (chess, football, etc --> elo based. team-based online games --> trueskill based or glicko based. coding contests --> ad-hoc, codeforces)
% - In general, the more complicated the rating system, the less adoptation.
% - Players overwhelmingly prefer the ability to understand and predict their own performance.
% - Rating systems are even used outside of sports: "Web-scale bayesian clickthrough
% rate prediction for sponsored search advertising in microsoftâ€™s bing search
% engine" pits ads against each other in a TrueSkill-like fashion.
% - The goal of a rating system is two-fold: firstly to predict the results between players, but also equally as important is to give the players an easily interpretable proxy for their skill.

% Brief literature review:
% - In programming contests the actual scores do not matter much, the ranks give the most discriminative information
% - Talk about area of pairwise comparisons
%     - Paired comparisons assume static "ability" throughout time.
%     - First to look at time-varying paired comparisons was the glicko rating system
% - Glicko and Glicko 2:
%     - Improvement upon Elo in that the deviation was also explicitly modelled
%     - A drift parameter was also added to increase rating volatility over time
%     - This in turn increases the rating uncertainty since the uncertainty is updated from the volatility
%     - Flaws famously exploited in pokemon go, as rating gain is proportional to rating uncertainty
% - MOV Elo techniques:
%     - These not only take in ranking as input, but also "margin of victory". This is appropriate for more fine-grained contests where the same type of contest is played over and over again (e.g. tennis). For programming contests this is not as useful since the contest changes over and over again. Methods not directly applicable in our setting.
%     - Streamlines interface between rating system and contest type and for contest authors. More applicable to other domains. Goodhart's Law.
% - IRT rating system:
%     - Tasks of a contest are modelled explicitly. Allows for more fine grained predictions (such as the number of problems solved during a contest)
% - WHR (Who-Rating-History): a bayesian approach to ratings that takes into account time-varying strength
%     - Three types of rating sys: Static, Incremental, Decayed-History
%     - Our rating system is Incremental. Inferior in the following sense: suppose that there are two players (A & B) that only play against each other. Then A plays against established players. This gives information to B's rating, but incremental systems leave B's rating unchanged.
%     - This rating system doesnt seem to necessarily have a monotonicity property: the addition of a game could decrease the ratings of some of the players in the system who do not directly participate in the game? Players are not typically happy about this; good motivation for our second algorithm property
%     - Limit of large # of players guarantee a connectivity property
% - TrueSkill and TrueSkill 2: 
%     - Models games where medium sized teams of players play against each other
%     - Each user has some skill drawn from a distribution
%     - The performance on any given day is drawn from a distribution centred at the skill
%     - The performance of a team is the sum of team skills
%     - A parameter eps is defined so that performances with difference less than eps is counted as a tie.
%     - Trained through message passing
%     - Used in massive scale in Halo 2
%     - Convergence properties unclear, hard to proof stuff about it
%     - Normal vs logistics support.
%     - TrueSkill 2 takes into account further application-specific statistics
% - Trueskill Through Time:
%     - Similar to WHR in that a new game affects whole history. Does this suffer from non-monotonicity?
% - TrueSkill StPb:
%     - Improved on trueskill by adapting factor graph to handle ties in a principled way
%     - Nodes are added to the factor graph grouping teams that are tied
%     - Instead of summing team performances, non-linear team performance functions are explored
% - TeamSkill and TeamSkill Evolved:
%     - An ensemble learner that leverages TrueSkill, Elo, and Glicko to better predict ratings in the presence of "team chemistry" and non-equal team sizes.
%     - However only works when teams play together semi-consistently
% - Codeforces & leetcode:
%     - Generalized Elo system
%     - Heuristics to fight against inflation
%     - O(n^2) time.
%     - Used in leetcode as well, somewhat of an industry standard
%     - total website use at least 3 million users
% - Topcoder:
%     - More similar to glicko in the sense that each user has a rating deviation
%     - Successfully attacked by Forisek

% The Elo rating system assigns a quantitative measure of skill to each player. For example, an average player may be rated 1500, while a top player may exceed 2500. The scale is arbitrary, but can be used to rank players relative to one another. The odds of any one player winning against another may be estimated from the difference in their ratings. The famous Elo system, and variants such as Glicko \cite{G99}, provide useful formulas for updating ratings of players who compete 1v1 against one another, resulting in a winner and a loser. These algorithms have some nice properties: they are fairly simple, fast, and only modify the ratings of the two participating players.

% Now let's consider a general setting in which a typical contest has much more than two participants. An arbitrary number of players compete simultaneously at a task. Rather than producing just a winner and a loser, the contest ranks its participants 1st place, 2nd, 3rd, and so on. This description fits popular programming competition websites such as Codeforces \cite{Codeforces} and Topcoder \cite{Topcoder}, each of which has tens of thousands of rated members from across the globe. Each publishes its own rating system, but without much theoretical justification to accompany the derivations.

% We build Elo-MMR upon a more rigorous probabilistic model, mirroring the Bayesian development of the Glicko system. In doing so, we inherit its nice properties in practice, resolving known issues with the Codeforces and Topcoder systems. Compared with these systems, it achieves faster convergence and robustness against unusual performances. Issues specific to Codeforces than we improve upon are the overall spread of ratings, inter-division boundary artifacts, and inflation (TODO: cite, and quantify this with tests). An issue specific to Topcoder that we eliminate completely is non-monotonicity: simply put, there are cases in which improving a member's past performance would actually decrease their Topcoder rating, and vice-versa \cite{forivsektheoretical}.

% Furthermore, Elo-MMR retains simplicity and efficiency on par with the other systems. To demonstrate this, I provide a very efficient parallel implementation that can process the entire history of rated competitions hosted by Codeforces on a modest quad-core laptop within 30 minutes. It is implemented entirely within the safe subset of Rust using the Rayon crate; hence, the Rust compiler verifies that it contains no data races.

% This paper is organized as follows: in section 2, we develop a Bayesian model for the competitions. Rating updates are phrased as a latent skill estimation problem, which is naturally divided into two phases. Sections 3 and 4 describe each of these phases in turn, and supplement the derivations with some intuitive interpretations. Then in section 5, we discuss some ways to model uncertainty, in a manner analogous to the Glicko system. In section 6, we discuss some properties of the Elo-MMR system in comparison with the Codeforces and Topcoder systems. Finally, section 7 presents the conclusions of this work.
% \end{comment}

Competitions, in the form of sports, games, and examinations, have been with us since antiquity. Many competitions grade performances along a numerical scale, such as a score on a test or a completion time in a race. In the case of a college admissions exam or a track race, scores are standardized so that a given score on two different occasions carries the same meaning. However, in events that feature novelty, subjectivity, or close interaction, standardization is difficult. The Spartan Races, completed by millions of runners, feature a variety of obstacles placed on hiking trails around the world~\cite{Spartan}. Rock climbing, a sport to be added to the 2020 Olympics, likewise has routes set specifically for each competition. DanceSport, gymnastics, and figure skating competitions have a panel of judges who rank contestants against one another; these subjective scores are known to be noisy~\cite{DanceSport}. 
%Most board games feature considerable inter-player interaction.
In all these cases, scores can only be used to compare and rank participants at the same event. Players, spectators, and contest organizers who are interested in comparing players' skill levels across different competitions will need to aggregate the entire history of such rankings. A strong player, then, is one who consistently wins against weaker players. To quantify skill, we need a \textbf{rating system}.


Good rating systems are difficult to create, as they must balance several mutually constraining objectives. First and foremost, rating systems must be accurate, in that ratings provide useful predictors of contest outcomes. Second, the ratings must be efficient to compute: within video game applications, rating systems are predominantly used for matchmaking in massively multiplayer online games (such as Halo, CounterStrike, League of Legends, etc.)~\cite{HMG06, MCZ18, Y14}. These games have hundreds of millions of players playing tens of millions of games per day, necessitating certain latency and memory requirements for the rating system~\cite{AL09}. Third, rating systems must be \textbf{incentive-compatible}: a player's rating should never increase had they scored worse, and never decrease had they scored better.
%should never depend inversely on an absolute score. 
This is to prevent players from regretting a win, or from throwing matches to game the system. Rating systems that can be gamed often create disastrous consequences to player-base, potentially leading to the loss of players~\cite{pokemongo}. Finally, the ratings provided by the system must be human-interpretable: ratings are typically represented to players as a single number encapsulating their overall skill, and many players want to understand and predict how their performances affect their rating~\cite{G95}.

% Should we consider something about robustness, and about ratings not changing when a player doesn't compete? There are two things that these systems have in common. First, all of the these systems propagate changes forward in time, never backward. This is a strict requirement of the system in most cases, as users prefer their historical ratings to state fixed (even if it's possible to infer more accurate historical ratings from future matches). Such a property will be a requirement of our system as well.

Classically, rating systems were designed for two-player games. The famous Elo system~\cite{E61}, as well as its Bayesian successors Glicko and Glicko-2, have been widely applied to games such as Chess and Go~\cite{G95, G99, G12}. Both Glicko versions model each player's skill as a real random variable that evolves with time according to Brownian motion. Inference is done by entering these variables into the Bradley-Terry model~\cite{BT52}, which predicts probabilities of game outcomes. Glicko-2 refines the Glicko system by adding a rating volatility parameter. Unfortunately, Glicko-2 is known to be flawed in practice, potentially incentivizing players to lose in what's known as ``volatility farming''. In some cases, these attacks can inflate a user's rating \emph{several hundred points} above its natural value, producing ratings that are essentially impossible to beat via honest play. This was most notably exploited in the popular game of Pokemon Go~\cite{pokemongo}. See \Cref{sec:mono} for a discussion of this issue, as well as an application of this attack to the Topcoder rating system.

The family of Elo-like methods just described only utilize the binary outcome of a match. In settings where a scoring system provides a more fine-grained measure of match performance, Kovalchik~\cite{K20} has shown variants of Elo that are able to take advantage of score information. For competitions consisting of several set tasks, such as academic olympiads, Fori{\v{s}}ek~\cite{forivsektheoretical} developed a model in which each task gives a different ``response'' to the player: the total response then predicts match outcomes. However, such systems are often highly application-dependent and hard to calibrate.

%In terms of predictive power, the main disadvantage of Elo-like methods is that they are ``history-less'' and ``static''. That is, previous competitions of an individual are forgotten and summarized solely through a few per-user parameters. Consider an example where two groups of users compete within their own groups and never outside of their group. To compare the two groups, we may choose a representative user from each group to compete against each other. Given the results of this match, it should be possible to calibrate the ratings of all the users in the two groups accordingly. However, Elo-like methods only update the ratings of the two users participating in the match. Coulom~\cite{C08} exploit this observation by retaining the entire match history of a user, allowing for more accurate predictions. 

Though Elo-like systems are widely used in two-player settings, one needn't look far to find competitions that involve much more than two players. In response to the popularity of team-based games such as CounterStrike and Halo, many recent works focus on competitions that are between two teams~\cite{HLW06, CJ16, LCFHH18, GFYLWTFC20}. Another popular setting is many-player contests such as academic olympiads: notably, programming contest platforms such as Codeforces, Topcoder, and Kaggle~\cite{Codeforces, Topcoder, Kaggle}. As with the aforementioned Spartan races, a typical event attracts thousands of contestants. Programming contest platforms have seen exponential growth over the past decade, collectively boasting millions of users~\cite{KaggleMilestone}. As an example, Codeforces gained over 200K new users in 2019 alone~\cite{CFResults}.
%but they do not present efficient extensions for settings in which players are sorted into more than two, let alone thousands, of distinct places. For these cases, a rating system must take in a final ranking of the competitors, and produce a list of rating updates. 

In ``free-for-all'' settings, where $N$ players are ranked individually, the Bayesian Approximation Ranking (BAR) algorithm~\cite{WL11} models the competition as a series of $\binom N2$ independent two-player contests. In reality, of course, the pairwise match outcomes are far from independent. Thus, TrueSkill~\cite{HMG06} and its variants~\cite{NS10, DHMG07, MCZ18} model a player's performance during each contest as a single random variable. The overall rankings are assumed to reveal the total order among these hidden performance variables, with various methods used to model ties and teams. For a textbook treatment of these methods, see~\cite{Winn19}. These rating systems are efficient in practice, successfully rating userbases that number well into the millions (the Halo series, for example, has over 60 million sales since 2001~\cite{Halo}).

The main disadvantage of TrueSkill is its complexity: originally developed by Microsoft for the popular Halo video game, TrueSkill performs approximate belief propagation, which consists of message passing on a factor graph, iterated until convergence. Aside from being less human-interpretable, this complexity means that, to our knowledge, there are no proofs of key properties such as runtime and incentive-compatibility. Even when these properties are discussed~\cite{MCZ18}, no rigorous justification is provided. In addition, we are not aware of any work that extends TrueSkill to non-Gaussian performance models, which might be desirable to limit the influence of outlier performances (see \Cref{sec:robust}).

It might be for these reasons that popular platforms such as Codeforces and Topcoder opted for their own custom rating systems. These systems are not published in academia and do not come with Bayesian justifications. However, they retain the formulaic simplicity of Elo and Glicko, extending them to settings with much more than two players. The Codeforces system includes ad hoc heuristics to distinguish top players, while curbing rampant inflation. Topcoder's formulas are more principled from a statistical perspective; however, it has a volatility parameter similar to Glicko-2, and hence suffers from similar exploits~\cite{forivsektheoretical}. Despite their flaws, these systems have been in place for over a decade, and have more recently gained adoption by additional platforms such as CodeChef and LeetCode~\cite{LeetCode, CodeChef}.


\paragraph{Our contributions} 
In this paper, we describe the Elo-MMR rating system, obtained by a principled approximation of a Bayesian model similar to Glicko and TrueSkill. It is fast, embarrassingly parallel, and makes accurate predictions. Most interesting of all, its simplicity allows us to rigorously analyze its properties: the ``MMR'' in the name stands for ``Massive'', ``Monotonic'', and ``Robust''. ``Massive'' means that it supports any number of players with a runtime that scales linearly; ``monotonic'' is a synonym for incentive-compatible, ensuring that a rating-maximizing player always wants to perform well; ``robust'' means that rating changes are bounded, with the bound being smaller for more consistent players than for volatile players. Robustness turns out to be a natural byproduct of accurately modeling performances with heavy-tailed distributions, such as the logistic. TrueSkill is believed to satisfy the first two properties, albeit without proof, but fails robustness. Codeforces only satisfies incentive-compatibility, and Topcoder only satisfies robustness.

Experimentally, we show that Elo-MMR achieves state-of-the-art performance in terms of both prediction accuracy and runtime on industry datasets. In particular, we process the entire Codeforces database of over 300K rated users and 1000 contests in well under a minute, beating the existing Codeforces system by an order of magnitude while improving upon its accuracy. Furthermore, we show that the well-known Topcoder system is severely vulnerable to volatility farming, whereas Elo-MMR is immune to such attacks. A difficulty we faced was the scarcity of efficient open-source rating system implementations. In an effort to aid researchers and practitioners alike, we provide open-source implementations of all rating systems, dataset mining, and additional processing used in our experiments at {\tt\url{https://github.com/EbTech/Elo-MMR}}.

\paragraph{Organization}
In \Cref{sec:bayes_model}, we formalize the details of our Bayesian model. We then show how to estimate player skill under this model in \Cref{sec:main-alg}, and develop some intuitions of the resulting formulas. As a further refinement, \Cref{sec:skill-drift} models skill evolutions from players training or atrophying between competitions. This modeling is quite tricky as we choose to retain players' momentum while preserving incentive-compatibility. While our modeling and derivations occupy multiple sections, the system itself is succinctly presented in \Cref{alg:main,alg:diffuse,alg:update}. In \Cref{sec:properties}, we perform a volatility farming attack on the Topcoder system and prove that, in contrast, Elo-MMR satisfies several salient properties, the most critical of which is incentive-compatibility. Finally, in \Cref{sec:experiments}, we present experimental evaluations, showing improvements over industry standards in both accuracy and speed. An extended version of this paper, with additional proofs and experiments, can be found at {\tt\url{https://arxiv.org/abs/2101.00400}}.